{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLhs4jVJHrXzvsj2fbpnyF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Setup & Dependencies"],"metadata":{"id":"VgBiNByBhatH"}},{"cell_type":"code","source":["!pip install torch torchvision torchmeta pillow pandas numpy transformers\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchmeta.datasets.helpers import miniimagenet\n","from torchmeta.utils.data import BatchMetaDataLoader\n","from transformers import ViTModel, ViTConfig\n","import numpy as np"],"metadata":{"id":"RSwIGiTchir-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Simplified Data Loading (Mock ISIC Data - Replace with Actual API Calls)"],"metadata":{"id":"k0mr_Bs2hlZU"}},{"cell_type":"code","source":["# For prototyping: Use MiniImagenet as stand-in for ISIC\n","train_dataset = miniimagenet(\"data\", ways=5, shots=5, test_shots=15, meta_train=True, download=True)\n","train_loader = BatchMetaDataLoader(train_dataset, batch_size=2, num_workers=2)"],"metadata":{"id":"zBJ2xUeNhr8v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Model Implementation"],"metadata":{"id":"Ek7yfkiqhybp"}},{"cell_type":"code","source":["class MetaRegViT(nn.Module):\n","    def __init__(self, num_base_classes=8, mask_ratio=0.2):\n","        super().__init__()\n","        self.mask_ratio = mask_ratio\n","\n","        # ViT-Tiny (6M params)\n","        config = ViTConfig(\n","            image_size=224, patch_size=16, num_classes=num_base_classes,\n","            hidden_size=128, num_hidden_layers=4, num_attention_heads=4\n","        )\n","        self.vit = ViTModel(config)\n","\n","        # Masking: Freeze 80% of attention heads\n","        self._freeze_heads()\n","\n","    def _freeze_heads(self):\n","        \"\"\"Freeze 80% of attention heads (static prototype)\"\"\"\n","        for layer in self.vit.encoder.layer:\n","            for head_idx in range(3):  # First 3/4 heads frozen\n","                param_names = [\n","                    f'attention.attention.query.weight_{head_idx}',\n","                    f'attention.attention.key.weight_{head_idx}',\n","                    f'attention.attention.value.weight_{head_idx}'\n","                ]\n","                for name, param in layer.named_parameters():\n","                    if any(p in name for p in param_names):\n","                        param.requires_grad = False\n","\n","    def forward(self, x):\n","        return self.vit(x).last_hidden_state[:, 0]  # CLS token"],"metadata":{"id":"QsGpb1eKh1U-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. MAML Training Loop (Phase 2)"],"metadata":{"id":"UXEJVp8ch7JT"}},{"cell_type":"code","source":["def maml_train(model, train_loader, epochs=100, inner_lr=0.01, outer_lr=3e-4):\n","    optimizer = optim.AdamW(model.parameters(), lr=outer_lr)\n","\n","    for epoch in range(epochs):\n","        for batch in train_loader:\n","            # Inner loop adaptation\n","            train_inputs, train_targets = batch[\"train\"]\n","            test_inputs, test_targets = batch[\"test\"]\n","\n","            # Clone model for task-specific adaptation\n","            learner = type(model)(num_base_classes=5)\n","            learner.load_state_dict(model.state_dict())\n","            inner_optim = optim.SGD(learner.parameters(), lr=inner_lr)\n","\n","            # Adapt on support set\n","            support_loss = nn.CrossEntropyLoss()(\n","                learner(train_inputs), train_targets\n","            )\n","            inner_optim.zero_grad()\n","            support_loss.backward()\n","            inner_optim.step()\n","\n","            # Outer loop update\n","            query_loss = nn.CrossEntropyLoss()(\n","                learner(test_inputs), test_targets\n","            )\n","            optimizer.zero_grad()\n","            query_loss.backward()\n","            optimizer.step()"],"metadata":{"id":"phMLYos6iUbc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Continual Learning with EWC (Phase 3)"],"metadata":{"id":"eZI6K45siXLy"}},{"cell_type":"code","source":["class EWC:\n","    def __init__(self, model, fisher_lambda=1e3):\n","        self.model = model\n","        self.fisher_lambda = fisher_lambda\n","        self.fisher = {}\n","        self.params = {n: p.clone() for n, p in model.named_parameters()}\n","\n","    def compute_fisher(self, dataset):\n","        # Simplified: Diagonal Fisher approximation\n","        for batch in dataset:\n","            loss = nn.CrossEntropyLoss()(self.model(batch[0]), batch[1])\n","            loss.backward()\n","            for n, p in self.model.named_parameters():\n","                if p.grad is not None:\n","                    self.fisher[n] = p.grad.data.clone().pow(2)\n","            break  # Use one batch for prototyping\n","\n","    def penalty(self):\n","        return sum(\n","            (self.fisher[n] * (p - self.params[n]).pow(2)).sum()\n","            for n, p in self.model.named_parameters()\n","        ) * self.fisher_lambda"],"metadata":{"id":"QxHA0_uxiZry"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Training Pipeline"],"metadata":{"id":"xX7uMSumii8t"}},{"cell_type":"code","source":["def main():\n","    # Phase 1: Base Training\n","    model = MetaRegViT(num_base_classes=8)\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.AdamW(model.parameters(), lr=3e-4)\n","\n","    # (Replace with actual base training loop)\n","    print(\"Skipping base training for prototyping...\")\n","\n","    # Phase 2: Meta-Learning\n","    maml_train(model, train_loader, epochs=10)\n","\n","    # Phase 3: Continual Learning\n","    ewc = EWC(model)\n","    ewc.compute_fisher(train_loader)  # Mock Fisher\n","\n","    # Mock incremental task\n","    incremental_optim = optim.SGD(model.parameters(), lr=1e-4)\n","    for _ in range(5):  # 5 epochs\n","        loss = criterion(model(torch.randn(2, 3, 224, 224)), torch.LongTensor([0, 1]))\n","        loss += ewc.penalty()\n","        incremental_optim.zero_grad()\n","        loss.backward()\n","        incremental_optim.step()\n","\n","    print(\"Training complete!\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"GyTJnbcuimYr"},"execution_count":null,"outputs":[]}]}