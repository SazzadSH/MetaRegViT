{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMw3DT3vyRHTAAvry0ykwq8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Setup & Dependencies"],"metadata":{"id":"fKbtpuGui54k"}},{"cell_type":"code","source":["!pip install tensorflow tensorflow-addons isic-api\n","import tensorflow as tf\n","from tensorflow.keras import layers, Model\n","import numpy as np\n","import tensorflow_addons as tfa"],"metadata":{"id":"vS5ih3Sji8hN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. Data Pipeline (Mock ISIC Data - Replace with Actual API Calls)"],"metadata":{"id":"kSiZ4YF9i-ae"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ISICLoader:\n","    def __init__(self, ways=5, shots=5):\n","        # Replace with ISIC API calls\n","        self.num_classes = ways\n","        self.shots = shots\n","\n","    def get_meta_batch(self, batch_size=4):\n","        # Generate mock meta-learning tasks\n","        return (\n","            tf.random.normal((batch_size, self.shots, 224, 224, 3)),  # Support\n","            tf.random.uniform((batch_size, self.shots), 0, self.num_classes, dtype=tf.int32),\n","            tf.random.normal((batch_size, 15, 224, 224, 3)),  # Query\n","            tf.random.uniform((batch_size, 15), 0, self.num_classes, dtype=tf.int32)\n","        )"],"metadata":{"id":"GqaTMA3ejIcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Model Implementation"],"metadata":{"id":"w5XE1eFrjMP2"}},{"cell_type":"code","source":["class MaskedMultiHeadAttention(layers.MultiHeadAttention):\n","    def __init__(self, mask_ratio=0.2, **kwargs):\n","        super().__init__(**kwargs)\n","        self.mask_ratio = mask_ratio\n","\n","    def _build_attention(self, rank):\n","        super()._build_attention(rank)\n","        # Initialize masking: 20% heads trainable\n","        self._trainable_heads = [int(i >= (1-self.mask_ratio)*self._num_heads)\n","                                for i in range(self._num_heads)]\n","\n","    def _compute_attention(self, *args, **kwargs):\n","        outputs = super()._compute_attention(*args, **kwargs)\n","        # Zero out non-trainable heads (simplified static masking)\n","        outputs *= tf.constant(self._trainable_heads, dtype=tf.float32)[None, None, :, None]\n","        return outputs\n","\n","class MetaRegViT(Model):\n","    def __init__(self, num_base_classes=8):\n","        super().__init__()\n","        self.patch_size = 16\n","\n","        # Input preprocessing\n","        self.patch_extract = layers.Conv2D(128, (16, 16), strides=16, activation='linear')\n","\n","        # Transformer Encoder with Masked Attention\n","        self.encoder = tf.keras.Sequential([\n","            tf.keras.Sequential([\n","                MaskedMultiHeadAttention(num_heads=4, key_dim=32),\n","                layers.Dense(128),  # FFN\n","            ]) for _ in range(4)\n","        ])\n","\n","        # Classification head\n","        self.head = layers.Dense(num_base_classes)\n","\n","    def call(self, inputs):\n","        # Patch embedding\n","        patches = self.patch_extract(inputs)\n","        batch = tf.shape(patches)[0]\n","        seq_len = (224//16)**2\n","        patches = tf.reshape(patches, (batch, seq_len, 128))\n","\n","        # Transformer encoder\n","        encoded = self.encoder(patches)\n","\n","        # CLS token prediction\n","        return self.head(encoded[:, 0])"],"metadata":{"id":"D6XXFZ1QjOXC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. MAML Training (Phase 2)"],"metadata":{"id":"0PKhW86RjTFz"}},{"cell_type":"code","source":["class MAMLTrainer:\n","    def __init__(self, model, inner_lr=0.01):\n","        self.model = model\n","        self.inner_lr = inner_lr\n","        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","    def train_step(self, support_x, support_y, query_x, query_y):\n","        # Inner loop\n","        task_weights = []\n","        for i in range(support_x.shape[0]):  # Batch dimension\n","            with tf.GradientTape() as tape:\n","                logits = self.model(support_x[i])\n","                loss = self.loss_fn(support_y[i], logits)\n","            grads = tape.gradient(loss, self.model.trainable_weights)\n","            task_weights.append([\n","                w - self.inner_lr * g for w, g in zip(self.model.weights, grads)\n","            ])\n","\n","        # Outer loop\n","        with tf.GradientTape() as outer_tape:\n","            total_loss = 0\n","            for i, weights in enumerate(task_weights):\n","                # Apply temporary weights\n","                original_weights = self.model.get_weights()\n","                self.model.set_weights(weights)\n","                query_logits = self.model(query_x[i])\n","                total_loss += self.loss_fn(query_y[i], query_logits)\n","                self.model.set_weights(original_weights)\n","\n","        # Update base model\n","        gradients = outer_tape.gradient(total_loss, self.model.trainable_weights)\n","        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n","        return total_loss"],"metadata":{"id":"kod-q-dHjVM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Continual Learning with EWC (Phase 3)"],"metadata":{"id":"bwUypNUGjdJ3"}},{"cell_type":"code","source":["class EWCRegularizer:\n","    def __init__(self, model, fisher_lambda=1e3):\n","        self.model = model\n","        self.fisher = {}\n","        self.opt_states = {}\n","        self.lambda_ = fisher_lambda\n","\n","    def compute_fisher(self, dataset):\n","        for name, var in self.model.named_trainable_weights:\n","            self.fisher[name] = tf.zeros_like(var)\n","\n","        for x, y in dataset.take(100):  # Use 100 samples\n","            with tf.GradientTape() as tape:\n","                logits = self.model(x)\n","                loss = self.loss_fn(y, logits)\n","            grads = tape.gradient(loss, self.model.trainable_weights)\n","            for name, g in zip(self.model.trainable_weights_names, grads):\n","                self.fisher[name] += g**2\n","\n","    def __call__(self):\n","        penalty = 0\n","        for name, var in self.model.named_trainable_weights:\n","            penalty += tf.reduce_sum(self.fisher[name] * (var - self.opt_states[name])**2)\n","        return self.lambda_ * penalty"],"metadata":{"id":"P6OTSXv7jfTH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Training Pipeline"],"metadata":{"id":"XXg5LhX6jhtB"}},{"cell_type":"code","source":["def main():\n","    # Initialize\n","    model = MetaRegViT()\n","    dataloader = ISICLoader()\n","    maml = MAMLTrainer(model)\n","    maml.optimizer = tf.keras.optimizers.Adam(3e-4)\n","\n","    # Phase 1: Base Training (Simplified)\n","    print(\"Skipping base pretraining...\")\n","\n","    # Phase 2: Meta-Learning\n","    for epoch in range(10):\n","        support_x, support_y, query_x, query_y = dataloader.get_meta_batch()\n","        loss = maml.train_step(support_x, support_y, query_x, query_y)\n","        print(f\"Meta-Epoch {epoch}: Loss={loss.numpy():.2f}\")\n","\n","    # Phase 3: Continual Learning with EWC\n","    ewc = EWCRegularizer(model)\n","    ewc.compute_fisher(tf.data.Dataset.from_tensor_slices((\n","        tf.random.normal((100, 224, 224, 3)),  # Mock data\n","        tf.random.uniform((100,), 0, 8, dtype=tf.int32)\n","    )).batch(4))\n","\n","    # Mock incremental task training\n","    opt = tf.keras.optimizers.SGD(1e-4)\n","    for _ in range(5):\n","        with tf.GradientTape() as tape:\n","            logits = model(tf.random.normal((4, 224, 224, 3)))\n","            loss = tf.keras.losses.sparse_categorical_crossentropy(\n","                tf.random.uniform((4,), 0, 8, dtype=tf.int32), logits, from_logits=True)\n","            loss += ewc()\n","        grads = tape.gradient(loss, model.trainable_weights)\n","        opt.apply_gradients(zip(grads, model.trainable_weights))\n","\n","    print(\"Training complete!\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"Sdid3-aojjnC"},"execution_count":null,"outputs":[]}]}